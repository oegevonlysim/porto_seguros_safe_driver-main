{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LightGBM Classifier for Porto Seguro Safe Driver Prediction\n",
        "\n",
        "This notebook implements a quick LightGBM classifier using default parameters on the Porto Seguro Safe Driver Prediction dataset. It includes functionality to train the model, evaluate performance, and visualize top feature importances.\n",
        "\n",
        "**Author:** GitHub Copilot  \n",
        "**Date:** 2025-11-12\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Import Required Libraries\n",
        "\n",
        "First, we import all necessary libraries for data manipulation, machine learning, and visualization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score, accuracy_score, classification_report\n",
        "from lightgbm import LGBMClassifier\n",
        "import warnings\n",
        "\n",
        "# Suppress warnings for cleaner output\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set plotting style\n",
        "sns.set_style('whitegrid')\n",
        "plt.rcParams['figure.figsize'] = (12, 8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Define Helper Functions\n",
        "\n",
        "We define functions to modularize the workflow for better code organization and reusability."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.1 Data Loading Function\n",
        "\n",
        "This function loads the Porto Seguro dataset or creates a mock dataset if the actual data is not available."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_data(data_path='data/train.csv', use_mock=False):\n",
        "    \"\"\"\n",
        "    Load the Porto Seguro dataset from the specified path.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    data_path : str\n",
        "        Path to the training data CSV file\n",
        "    use_mock : bool\n",
        "        If True, creates a mock dataset for demonstration purposes\n",
        "        \n",
        "    Returns:\n",
        "    --------\n",
        "    X : pd.DataFrame\n",
        "        Feature matrix\n",
        "    y : pd.Series\n",
        "        Target variable\n",
        "    feature_names : list\n",
        "        List of feature column names\n",
        "    \"\"\"\n",
        "    print(\"=\" * 80)\n",
        "    print(\"STEP 1: Loading Data\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    # Check if the actual data file exists\n",
        "    if not use_mock and os.path.exists(data_path):\n",
        "        print(f\"\u2713 Loading data from: {data_path}\")\n",
        "        df = pd.read_csv(data_path)\n",
        "        print(f\"\u2713 Data loaded successfully!\")\n",
        "        print(f\"  - Dataset shape: {df.shape}\")\n",
        "        print(f\"  - Columns: {df.shape[1]}\")\n",
        "        print(f\"  - Rows: {df.shape[0]}\")\n",
        "        \n",
        "        # Separate features and target\n",
        "        # Drop 'id' column as it's not a feature\n",
        "        X = df.drop(['id', 'target'], axis=1, errors='ignore')\n",
        "        y = df['target']\n",
        "        \n",
        "        print(f\"\\n\u2713 Using ACTUAL Porto Seguro dataset\")\n",
        "        \n",
        "    else:\n",
        "        # Create a mock dataset for demonstration\n",
        "        print(\"\u26a0 Actual dataset not found. Creating MOCK dataset for demonstration...\")\n",
        "        np.random.seed(42)\n",
        "        n_samples = 1000\n",
        "        n_features = 20\n",
        "        \n",
        "        # Generate random features\n",
        "        X = pd.DataFrame(\n",
        "            np.random.randn(n_samples, n_features),\n",
        "            columns=[f'feature_{i}' for i in range(n_features)]\n",
        "        )\n",
        "        \n",
        "        # Generate target variable (binary classification)\n",
        "        y = pd.Series(np.random.randint(0, 2, n_samples), name='target')\n",
        "        \n",
        "        print(f\"\u2713 Mock dataset created!\")\n",
        "        print(f\"  - Dataset shape: {X.shape}\")\n",
        "        print(f\"  - Features: {n_features}\")\n",
        "        print(f\"  - Samples: {n_samples}\")\n",
        "        print(f\"\\n\u26a0 NOTE: This is a MOCK dataset, not the actual Porto Seguro data!\")\n",
        "    \n",
        "    feature_names = X.columns.tolist()\n",
        "    \n",
        "    print(f\"\\nTarget distribution:\")\n",
        "    print(y.value_counts())\n",
        "    print(f\"Class balance: {y.value_counts(normalize=True).to_dict()}\")\n",
        "    \n",
        "    return X, y, feature_names"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.2 Data Preparation Function\n",
        "\n",
        "This function handles missing values and splits the data into training and testing sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def prepare_data(X, y, test_size=0.2, random_state=42):\n",
        "    \"\"\"\n",
        "    Split the data into training and testing sets.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    X : pd.DataFrame\n",
        "        Feature matrix\n",
        "    y : pd.Series\n",
        "        Target variable\n",
        "    test_size : float\n",
        "        Proportion of data to use for testing (default: 0.2)\n",
        "    random_state : int\n",
        "        Random seed for reproducibility (default: 42)\n",
        "        \n",
        "    Returns:\n",
        "    --------\n",
        "    X_train, X_test, y_train, y_test : tuple\n",
        "        Training and testing splits\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"STEP 2: Preparing Data\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    # Handle missing values by filling with -999 (common strategy for tree-based models)\n",
        "    print(\"\u2713 Handling missing values...\")\n",
        "    X_filled = X.fillna(-999)\n",
        "    missing_count = X.isnull().sum().sum()\n",
        "    if missing_count > 0:\n",
        "        print(f\"  - Filled {missing_count} missing values with -999\")\n",
        "    else:\n",
        "        print(f\"  - No missing values found\")\n",
        "    \n",
        "    # Split data\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X_filled, y, test_size=test_size, random_state=random_state, stratify=y\n",
        "    )\n",
        "    \n",
        "    print(f\"\\n\u2713 Data split completed!\")\n",
        "    print(f\"  - Training set: {X_train.shape[0]} samples ({(1-test_size)*100:.0f}%)\")\n",
        "    print(f\"  - Test set: {X_test.shape[0]} samples ({test_size*100:.0f}%)\")\n",
        "    print(f\"  - Number of features: {X_train.shape[1]}\")\n",
        "    \n",
        "    return X_train, X_test, y_train, y_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.3 Model Training Function\n",
        "\n",
        "This function trains a LightGBM classifier with default parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_model(X_train, y_train):\n",
        "    \"\"\"\n",
        "    Train a LightGBM classifier with default parameters.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    X_train : pd.DataFrame\n",
        "        Training feature matrix\n",
        "    y_train : pd.Series\n",
        "        Training target variable\n",
        "        \n",
        "    Returns:\n",
        "    --------\n",
        "    model : LGBMClassifier\n",
        "        Trained LightGBM model\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"STEP 3: Training LightGBM Classifier\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    print(\"\u2713 Initializing LGBMClassifier with default parameters...\")\n",
        "    print(\"  - Using default LightGBM hyperparameters\")\n",
        "    print(\"  - This is a quick baseline model\")\n",
        "    \n",
        "    # Initialize the model with default parameters\n",
        "    # Setting verbose=-1 to suppress training output\n",
        "    model = LGBMClassifier(random_state=42, verbose=-1)\n",
        "    \n",
        "    print(\"\\n\u2713 Training the model...\")\n",
        "    model.fit(X_train, y_train)\n",
        "    \n",
        "    print(\"\u2713 Model training completed!\")\n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.4 Model Evaluation Function\n",
        "\n",
        "This function evaluates the trained model on both training and test sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_model(model, X_train, y_train, X_test, y_test):\n",
        "    \"\"\"\n",
        "    Evaluate the trained model on both training and test sets.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    model : LGBMClassifier\n",
        "        Trained model\n",
        "    X_train : pd.DataFrame\n",
        "        Training feature matrix\n",
        "    y_train : pd.Series\n",
        "        Training target variable\n",
        "    X_test : pd.DataFrame\n",
        "        Test feature matrix\n",
        "    y_test : pd.Series\n",
        "        Test target variable\n",
        "        \n",
        "    Returns:\n",
        "    --------\n",
        "    metrics : dict\n",
        "        Dictionary containing evaluation metrics\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"STEP 4: Evaluating Model Performance\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    # Make predictions\n",
        "    y_train_pred = model.predict(X_train)\n",
        "    y_test_pred = model.predict(X_test)\n",
        "    \n",
        "    # Predict probabilities for ROC-AUC calculation\n",
        "    y_train_pred_proba = model.predict_proba(X_train)[:, 1]\n",
        "    y_test_pred_proba = model.predict_proba(X_test)[:, 1]\n",
        "    \n",
        "    # Calculate metrics\n",
        "    train_accuracy = accuracy_score(y_train, y_train_pred)\n",
        "    test_accuracy = accuracy_score(y_test, y_test_pred)\n",
        "    train_roc_auc = roc_auc_score(y_train, y_train_pred_proba)\n",
        "    test_roc_auc = roc_auc_score(y_test, y_test_pred_proba)\n",
        "    \n",
        "    print(\"\\n\ud83d\udcca Performance Metrics:\")\n",
        "    print(\"-\" * 80)\n",
        "    print(f\"Training Set:\")\n",
        "    print(f\"  - Accuracy:  {train_accuracy:.4f}\")\n",
        "    print(f\"  - ROC-AUC:   {train_roc_auc:.4f}\")\n",
        "    print(f\"\\nTest Set:\")\n",
        "    print(f\"  - Accuracy:  {test_accuracy:.4f}\")\n",
        "    print(f\"  - ROC-AUC:   {test_roc_auc:.4f}\")\n",
        "    \n",
        "    print(\"\\n\ud83d\udccb Classification Report (Test Set):\")\n",
        "    print(\"-\" * 80)\n",
        "    print(classification_report(y_test, y_test_pred))\n",
        "    \n",
        "    metrics = {\n",
        "        'train_accuracy': train_accuracy,\n",
        "        'test_accuracy': test_accuracy,\n",
        "        'train_roc_auc': train_roc_auc,\n",
        "        'test_roc_auc': test_roc_auc\n",
        "    }\n",
        "    \n",
        "    return metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.5 Feature Importance Visualization Function\n",
        "\n",
        "This function creates a visualization of the most important features identified by the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_feature_importance(model, feature_names, top_n=20, save_path='feature_importance.png'):\n",
        "    \"\"\"\n",
        "    Plot the top N feature importances from the trained model.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    model : LGBMClassifier\n",
        "        Trained LightGBM model\n",
        "    feature_names : list\n",
        "        List of feature names\n",
        "    top_n : int\n",
        "        Number of top features to display (default: 20)\n",
        "    save_path : str\n",
        "        Path to save the plot image (default: 'feature_importance.png')\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"STEP 5: Plotting Feature Importances\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    # Get feature importances\n",
        "    importances = model.feature_importances_\n",
        "    \n",
        "    # Create a DataFrame for easier manipulation\n",
        "    feature_importance_df = pd.DataFrame({\n",
        "        'feature': feature_names,\n",
        "        'importance': importances\n",
        "    }).sort_values('importance', ascending=False)\n",
        "    \n",
        "    # Select top N features\n",
        "    top_features = feature_importance_df.head(top_n)\n",
        "    \n",
        "    print(f\"\\n\u2713 Top {top_n} Most Important Features:\")\n",
        "    print(\"-\" * 80)\n",
        "    for idx, row in top_features.iterrows():\n",
        "        print(f\"  {row['feature']:30s} : {row['importance']:.2f}\")\n",
        "    \n",
        "    # Create the plot\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    \n",
        "    # Create horizontal bar plot\n",
        "    colors = plt.cm.viridis(np.linspace(0, 1, len(top_features)))\n",
        "    bars = plt.barh(range(len(top_features)), top_features['importance'], color=colors)\n",
        "    \n",
        "    # Customize the plot\n",
        "    plt.yticks(range(len(top_features)), top_features['feature'])\n",
        "    plt.xlabel('Feature Importance', fontsize=12, fontweight='bold')\n",
        "    plt.ylabel('Features', fontsize=12, fontweight='bold')\n",
        "    plt.title(f'Top {top_n} Feature Importances - LightGBM Classifier', \n",
        "              fontsize=14, fontweight='bold', pad=20)\n",
        "    plt.gca().invert_yaxis()  # Highest importance at the top\n",
        "    \n",
        "    # Add value labels on the bars\n",
        "    for i, (bar, value) in enumerate(zip(bars, top_features['importance'])):\n",
        "        plt.text(value, i, f' {value:.2f}', \n",
        "                va='center', fontsize=9, fontweight='bold')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    \n",
        "    # Save the plot\n",
        "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "    print(f\"\\n\u2713 Feature importance plot saved to: {save_path}\")\n",
        "    \n",
        "    # Display the plot\n",
        "    plt.show()\n",
        "    print(\"\u2713 Plot displayed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 3. Execute the Workflow\n",
        "\n",
        "Now we'll execute the complete workflow step by step."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 1: Load the Data\n",
        "\n",
        "Load the Porto Seguro dataset from the data directory. The function will automatically create a mock dataset if the actual data is not available."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load data\n",
        "data_path = 'data/train.csv'\n",
        "use_mock = not os.path.exists(data_path)\n",
        "\n",
        "X, y, feature_names = load_data(data_path=data_path, use_mock=use_mock)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 2: Prepare the Data\n",
        "\n",
        "Split the data into training and testing sets with an 80-20 split."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare data (train-test split)\n",
        "X_train, X_test, y_train, y_test = prepare_data(X, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 3: Train the Model\n",
        "\n",
        "Train a LightGBM classifier using default parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train the model\n",
        "model = train_model(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 4: Evaluate the Model\n",
        "\n",
        "Evaluate model performance on both training and test sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate the model\n",
        "metrics = evaluate_model(model, X_train, y_train, X_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 5: Visualize Feature Importances\n",
        "\n",
        "Create a visualization showing the top 20 most important features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot feature importances\n",
        "plot_feature_importance(model, feature_names, top_n=20, save_path='feature_importance.png')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 4. Summary\n",
        "\n",
        "Print a final summary of the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"\u2713 All steps completed successfully!\")\n",
        "print(\"=\" * 80)\n",
        "print(\"\\n\ud83d\udcdd Summary:\")\n",
        "print(f\"  - Model: LightGBM Classifier (default parameters)\")\n",
        "print(f\"  - Test Accuracy: {metrics['test_accuracy']:.4f}\")\n",
        "print(f\"  - Test ROC-AUC: {metrics['test_roc_auc']:.4f}\")\n",
        "print(f\"  - Feature importance plot saved: feature_importance.png\")\n",
        "print(\"\\n\" + \"=\" * 80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 5. Next Steps\n",
        "\n",
        "This notebook provides a baseline model using default parameters. Here are some suggestions for improvements:\n",
        "\n",
        "1. **Hyperparameter Tuning**: Use GridSearchCV or Optuna to optimize parameters\n",
        "2. **Feature Engineering**: Create interaction features, polynomial features\n",
        "3. **Handle Class Imbalance**: Use class weights, SMOTE, or undersampling\n",
        "4. **Feature Selection**: Remove low-importance or calc_* features\n",
        "5. **Ensemble Methods**: Combine multiple models for better performance\n",
        "\n",
        "For more details, see the [LGBM_README.md](LGBM_README.md) documentation."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}