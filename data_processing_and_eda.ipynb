{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Porto Seguro Safe Driver Prediction - Data Processing and EDA\n",
    "\n",
    "This notebook provides a starting point for processing and analyzing the Porto Seguro Safe Driver Prediction dataset.\n",
    "\n",
    "## Sections:\n",
    "1. **Setup and Data Extraction** - Import libraries and extract the dataset from the zip file\n",
    "2. **Data Loading** - Read the extracted CSV files and display their structure\n",
    "3. **Exploratory Data Analysis** - Perform basic EDA including summary statistics and visualizations\n",
    "4. **Initial Insights** - Document key findings from the exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Data Extraction\n",
    "\n",
    "First, we'll import the necessary libraries and extract the dataset from the zip file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import zipfile\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "# Configure visualization settings\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define file paths\n",
    "zip_file_path = 'data/porto-seguro-safe-driver-prediction.zip'\n",
    "extraction_path = 'data/'\n",
    "\n",
    "# Check if zip file exists\n",
    "if os.path.exists(zip_file_path):\n",
    "    print(f\"Zip file found: {zip_file_path}\")\n",
    "    print(f\"File size: {os.path.getsize(zip_file_path) / (1024**2):.2f} MB\")\n",
    "else:\n",
    "    print(f\"Error: Zip file not found at {zip_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the zip file\n",
    "print(\"Extracting zip file...\")\n",
    "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "    # List contents of the zip file\n",
    "    file_list = zip_ref.namelist()\n",
    "    print(f\"\\nFiles in archive: {file_list}\")\n",
    "    \n",
    "    # Extract all files\n",
    "    zip_ref.extractall(extraction_path)\n",
    "    print(f\"\\nFiles extracted to: {extraction_path}\")\n",
    "\n",
    "# Verify extraction\n",
    "extracted_files = [f for f in os.listdir(extraction_path) if f.endswith('.csv')]\n",
    "print(f\"\\nExtracted CSV files: {extracted_files}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Structure\n",
    "\n",
    "Now we'll load the extracted data files and examine their structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training dataset\n",
    "print(\"Loading training data...\")\n",
    "train_df = pd.read_csv(os.path.join(extraction_path, 'train.csv'))\n",
    "print(f\"Training data loaded successfully!\")\n",
    "print(f\"Shape: {train_df.shape}\")\n",
    "print(f\"Rows: {train_df.shape[0]:,}, Columns: {train_df.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first few rows\n",
    "print(\"First 5 rows of the training dataset:\\n\")\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display column information\n",
    "print(\"Dataset Information:\\n\")\n",
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display column names and their data types\n",
    "print(\"Column Names and Data Types:\\n\")\n",
    "column_info = pd.DataFrame({\n",
    "    'Column': train_df.columns,\n",
    "    'Data Type': train_df.dtypes.values,\n",
    "    'Non-Null Count': train_df.count().values,\n",
    "    'Null Count': train_df.isnull().sum().values\n",
    "})\n",
    "column_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test dataset (for reference)\n",
    "print(\"Loading test data...\")\n",
    "test_df = pd.read_csv(os.path.join(extraction_path, 'test.csv'))\n",
    "print(f\"Test data loaded successfully!\")\n",
    "print(f\"Shape: {test_df.shape}\")\n",
    "print(f\"Rows: {test_df.shape[0]:,}, Columns: {test_df.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Exploratory Data Analysis (EDA)\n",
    "\n",
    "Let's perform comprehensive exploratory data analysis to understand the dataset better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate summary statistics for all numerical columns\n",
    "print(\"Summary Statistics:\\n\")\n",
    "train_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the target variable distribution\n",
    "print(\"Target Variable Distribution:\\n\")\n",
    "print(train_df['target'].value_counts())\n",
    "print(f\"\\nTarget Distribution (%):\\n{train_df['target'].value_counts(normalize=True) * 100}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Missing Values Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "missing_values = train_df.isnull().sum()\n",
    "missing_percentage = (missing_values / len(train_df)) * 100\n",
    "\n",
    "missing_df = pd.DataFrame({\n",
    "    'Column': train_df.columns,\n",
    "    'Missing Count': missing_values.values,\n",
    "    'Missing Percentage': missing_percentage.values\n",
    "})\n",
    "\n",
    "# Show only columns with missing values\n",
    "missing_df_filtered = missing_df[missing_df['Missing Count'] > 0].sort_values('Missing Count', ascending=False)\n",
    "\n",
    "if len(missing_df_filtered) > 0:\n",
    "    print(\"Columns with Missing Values:\\n\")\n",
    "    print(missing_df_filtered)\n",
    "else:\n",
    "    print(\"No missing values found in the dataset!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Feature Analysis\n",
    "\n",
    "The dataset contains different types of features:\n",
    "- **Binary features** (suffix: _bin)\n",
    "- **Categorical features** (suffix: _cat)\n",
    "- **Continuous/Ordinal features** (no suffix or other suffixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorize features by type\n",
    "binary_features = [col for col in train_df.columns if '_bin' in col]\n",
    "categorical_features = [col for col in train_df.columns if '_cat' in col]\n",
    "continuous_features = [col for col in train_df.columns \n",
    "                       if col not in binary_features + categorical_features + ['id', 'target']]\n",
    "\n",
    "print(f\"Binary features ({len(binary_features)}): {binary_features[:5]}...\")\n",
    "print(f\"\\nCategorical features ({len(categorical_features)}): {categorical_features[:5]}...\")\n",
    "print(f\"\\nContinuous/Ordinal features ({len(continuous_features)}): {continuous_features[:5]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot target variable distribution\n",
    "fig, ax = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Count plot\n",
    "train_df['target'].value_counts().plot(kind='bar', ax=ax[0], color=['skyblue', 'salmon'])\n",
    "ax[0].set_title('Target Variable Distribution (Count)', fontsize=14, fontweight='bold')\n",
    "ax[0].set_xlabel('Target', fontsize=12)\n",
    "ax[0].set_ylabel('Count', fontsize=12)\n",
    "ax[0].set_xticklabels(['No Claim (0)', 'Claim (1)'], rotation=0)\n",
    "\n",
    "# Pie chart\n",
    "train_df['target'].value_counts().plot(kind='pie', ax=ax[1], autopct='%1.1f%%', \n",
    "                                        colors=['skyblue', 'salmon'], startangle=90)\n",
    "ax[1].set_title('Target Variable Distribution (%)', fontsize=14, fontweight='bold')\n",
    "ax[1].set_ylabel('')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nNote: The dataset is imbalanced with {(train_df['target']==0).sum()} non-claimants and {(train_df['target']==1).sum()} claimants.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of some continuous features\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "# Select first 6 continuous features for visualization\n",
    "features_to_plot = continuous_features[:6]\n",
    "\n",
    "for idx, feature in enumerate(features_to_plot):\n",
    "    axes[idx].hist(train_df[feature].dropna(), bins=50, color='steelblue', edgecolor='black', alpha=0.7)\n",
    "    axes[idx].set_title(f'Distribution of {feature}', fontsize=12, fontweight='bold')\n",
    "    axes[idx].set_xlabel(feature, fontsize=10)\n",
    "    axes[idx].set_ylabel('Frequency', fontsize=10)\n",
    "    axes[idx].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation analysis - select a subset of features for better visualization\n",
    "# We'll look at correlations among continuous features\n",
    "features_for_corr = continuous_features[:10]  # First 10 continuous features\n",
    "correlation_matrix = train_df[features_for_corr + ['target']].corr()\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm', \n",
    "            square=True, linewidths=0.5, cbar_kws={\"shrink\": 0.8})\n",
    "plt.title('Correlation Matrix (Sample Features)', fontsize=14, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature correlation with target variable\n",
    "target_correlations = train_df.corr()['target'].sort_values(ascending=False)\n",
    "\n",
    "# Plot top 15 positive and negative correlations\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 6))\n",
    "\n",
    "# Top positive correlations\n",
    "top_positive = target_correlations[1:16]  # Exclude target itself\n",
    "top_positive.plot(kind='barh', ax=axes[0], color='green', alpha=0.7)\n",
    "axes[0].set_title('Top 15 Positive Correlations with Target', fontsize=12, fontweight='bold')\n",
    "axes[0].set_xlabel('Correlation Coefficient', fontsize=10)\n",
    "\n",
    "# Top negative correlations\n",
    "top_negative = target_correlations[-15:]\n",
    "top_negative.plot(kind='barh', ax=axes[1], color='red', alpha=0.7)\n",
    "axes[1].set_title('Top 15 Negative Correlations with Target', fontsize=12, fontweight='bold')\n",
    "axes[1].set_xlabel('Correlation Coefficient', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze binary features\n",
    "if len(binary_features) > 0:\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "    axes = axes.ravel()\n",
    "    \n",
    "    # Plot first 6 binary features\n",
    "    for idx, feature in enumerate(binary_features[:6]):\n",
    "        train_df[feature].value_counts().plot(kind='bar', ax=axes[idx], color='teal', alpha=0.7)\n",
    "        axes[idx].set_title(f'{feature} Distribution', fontsize=12, fontweight='bold')\n",
    "        axes[idx].set_xlabel('Value', fontsize=10)\n",
    "        axes[idx].set_ylabel('Count', fontsize=10)\n",
    "        axes[idx].set_xticklabels(axes[idx].get_xticklabels(), rotation=0)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze categorical features - show unique values\n",
    "if len(categorical_features) > 0:\n",
    "    print(\"Categorical Features - Unique Value Counts:\\n\")\n",
    "    cat_summary = pd.DataFrame({\n",
    "        'Feature': categorical_features[:10],  # First 10 categorical features\n",
    "        'Unique Values': [train_df[col].nunique() for col in categorical_features[:10]]\n",
    "    })\n",
    "    print(cat_summary)\n",
    "    \n",
    "    # Plot unique value counts\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    cat_summary.plot(x='Feature', y='Unique Values', kind='bar', \n",
    "                     color='purple', alpha=0.7, legend=False)\n",
    "    plt.title('Number of Unique Values in Categorical Features', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Feature', fontsize=12)\n",
    "    plt.ylabel('Unique Value Count', fontsize=12)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Initial Insights\n",
    "\n",
    "Based on the exploratory data analysis, here are some key observations:\n",
    "\n",
    "### Dataset Characteristics:\n",
    "- The dataset contains insurance claim data with multiple feature types (binary, categorical, and continuous)\n",
    "- Features are anonymized with generic names (ps_)\n",
    "- The target variable indicates whether a claim was filed (1) or not (0)\n",
    "\n",
    "### Key Findings:\n",
    "1. **Class Imbalance**: The target variable shows significant class imbalance - claims are much less frequent than non-claims\n",
    "2. **Feature Types**: The dataset contains:\n",
    "   - Binary features (suffix: _bin)\n",
    "   - Categorical features (suffix: _cat)\n",
    "   - Continuous/ordinal features\n",
    "3. **Missing Values**: Check if any features have missing values that need handling\n",
    "4. **Feature Correlations**: Some features show correlation with the target variable\n",
    "\n",
    "### Next Steps:\n",
    "- Handle missing values appropriately\n",
    "- Address class imbalance using techniques like SMOTE, class weights, or sampling\n",
    "- Feature engineering and selection\n",
    "- Model selection and training\n",
    "- Evaluation using appropriate metrics for imbalanced classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook provided:\n",
    "1. ✅ Extraction of the Porto Seguro dataset from the zip file\n",
    "2. ✅ Loading and displaying the structure of the data\n",
    "3. ✅ Comprehensive exploratory data analysis including:\n",
    "   - Summary statistics\n",
    "   - Missing value analysis\n",
    "   - Feature type categorization\n",
    "   - Distribution visualizations\n",
    "   - Correlation analysis\n",
    "4. ✅ Documentation and insights for each analysis step\n",
    "\n",
    "This notebook serves as a foundation for further analysis and modeling work on the Porto Seguro Safe Driver Prediction dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
